# Ultralytics YOLO ğŸš€, AGPL-3.0 license
"""
Model head modules

æ­¤æ–‡ä»¶å®ç°å¤šç§ headï¼ˆæ£€æµ‹/åˆ†å‰²/å…³é”®ç‚¹/è”åˆï¼‰ï¼š
- Detect: åŸºç¡€æ£€æµ‹å¤´ï¼Œè¾“å‡º bbox åˆ†å¸ƒå›å½’ + ç±»åˆ« logitsï¼Œæ”¯æŒ DFL åˆ†å¸ƒå›å½’ã€‚
- Segment: åœ¨ Detect åŸºç¡€ä¸Šå¢åŠ  protoï¼ˆåŸå‹ maskï¼‰ä¸ mask coefficientsï¼Œç”¨äºå®ä¾‹åˆ†å‰²ã€‚
- Pose: åœ¨ Detect åŸºç¡€ä¸Šå¢åŠ  keypoint åˆ†æ”¯å¹¶å®ç°è§£ç ã€‚
- SegmentPose: åŒæ—¶åŒ…å« Segment å’Œ Pose çš„åˆ†æ”¯ï¼Œåœ¨å•ä¸€å‰å‘è¿‡ç¨‹ä¸­è¾“å‡º bbox/class + mask coeff + keypointsã€‚
æ­¤å¤–è¿˜æœ‰ç®€å•çš„ Classify å’Œä¸€ä¸ªåŸºäº Deformable Transformer çš„ RTDETRDecoderï¼ˆquery-based decoderï¼‰ã€‚
"""

import math

import torch
import torch.nn as nn
from torch.nn.init import constant_, xavier_uniform_

# å·¥å…·å‡½æ•°ï¼šdist2bbox ç”¨äºæŠŠåˆ†å¸ƒ/åç§»è½¬æ¢ä¸º bboxï¼Œmake_anchors ç”¨äºæ„é€ ç½‘æ ¼ anchorï¼ˆä¸­å¿ƒåæ ‡ï¼‰
from ultralytics.yolo.utils.tal import dist2bbox, make_anchors
# make_anchorsï¼šæ ¹æ®è¾“å‡ºç‰¹å¾å›¾å¤§å°å’Œ stride ç”Ÿæˆç½‘æ ¼é”šç‚¹ï¼ˆanchor ä¸­å¿ƒä¸ strideï¼‰
# dist2bboxï¼šå°† DFL çš„åˆ†å¸ƒè¡¨ç¤ºæˆ–ç›¸å¯¹é‡è½¬æˆçœŸæ­£çš„æ¡†ï¼ˆxywhï¼‰

# å±€éƒ¨æ¨¡å—ï¼šDFLï¼ˆdistribution focal loss / distribution based regressionï¼‰ã€Protoï¼ˆmask åŸå‹ç½‘ç»œï¼‰
# DFLï¼šdistribution focal layerï¼Œå°† reg_max çš„ logitsè½¬ä¸ºåç§»é‡ï¼ˆé€šå¸¸å…ˆ softmax å†æœŸæœ›å€¼ï¼‰ã€‚ä¼ ç»Ÿ bbox å›å½’ç›´æ¥å›å½’ 4 ä¸ªå®æ•°ã€‚DFL ç”¨å°†æ¯ä¸ªåæ ‡åˆ†æˆå¤šä¸ªç¦»æ•£ binï¼ˆreg_maxï¼‰å­¦ä¹ ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œå†é€šè¿‡æœŸæœ›æˆ–ç±»ä¼¼æ–¹æ³•å¾—åˆ°è¿ç»­å€¼ï¼Œèƒ½æé«˜å›å½’ç²¾åº¦ä¸ç¨³å®šæ€§ï¼ˆå°¤å…¶å¯¹å°ç‰©ä½“æˆ–é«˜ç²¾åº¦éœ€æ±‚æœ‰å¸®åŠ©ï¼‰
from .block import DFL, Proto
# Conv æ˜¯å°è£…çš„å·ç§¯æ¨¡å—ï¼ˆå« BN/æ¿€æ´»ç­‰ï¼‰
from .conv import Conv
# Transformer ç›¸å…³ç®€åŒ–æ¨¡å—ï¼ˆç”¨äº RTDETRDecoderï¼‰
from .transformer import MLP, DeformableTransformerDecoder, DeformableTransformerDecoderLayer
# åˆå§‹åŒ–è¾…åŠ©
from .utils import bias_init_with_prob, linear_init_

__all__ = 'Detect', 'Segment', 'Pose', 'Classify', 'RTDETRDecoder'


class Detect(nn.Module):
    """YOLOv8 Detect head for detection models.

    åŠŸèƒ½æ€»ç»“ï¼š
    - æ¯ä¸ªç‰¹å¾å°ºåº¦é€šè¿‡ä¸¤ä¸ªåˆ†æ”¯è¾“å‡ºï¼šbox åˆ†å¸ƒ (4 * reg_max) å’Œç±»åˆ« logits (nc)ã€‚
    - ä½¿ç”¨ DFL å°†ç¦»æ•£åˆ†å¸ƒè½¬æ¢ä¸ºè¿ç»­åç§»ï¼ˆå¦‚æœ reg_max > 1ï¼‰ã€‚
    - åœ¨è®­ç»ƒæ—¶ç›´æ¥è¿”å›å„å°ºåº¦çš„åŸå§‹ logitsï¼ˆä¾¿äº loss è®¡ç®—ï¼‰ã€‚
    - åœ¨æ¨ç†æ—¶æ„é€  anchors/stridesã€å°†è¾“å‡ºæ‹¼æ¥ã€å¯¹ box éƒ¨åˆ†åš dfl->bbox è§£ç å¹¶æ‹¼æ¥ sigmoid åçš„ç±»åˆ«æ¦‚ç‡è¿”å›ã€‚
    """

    dynamic = False  # æ˜¯å¦å¼ºåˆ¶åœ¨æ¯æ¬¡å‰å‘é‡å»º gridï¼ˆanchorsï¼‰ï¼Œç”¨äºåŠ¨æ€å›¾åœºæ™¯
    export = False  # å¯¼å‡ºæ¨¡å‹ï¼ˆtflite/tfjs/saved_modelï¼‰æ—¶çš„ä¸€äº›ç‰¹æ®Šå¤„ç†æ ‡å¿—
    shape = None
    anchors = torch.empty(0)  # anchors å ä½ï¼ˆåœ¨ç¬¬ä¸€æ¬¡æ¨ç†æˆ–è°ƒç”¨ make_anchors æ—¶å¡«å……ï¼‰
    strides = torch.empty(0)  # strides å ä½

    def __init__(self, nc=80, ch=()):  # detection layer
        super().__init__()
        self.nc = nc  # ç±»åˆ«æ•°é‡
        self.nl = len(ch)  # detection å±‚æ•°ï¼ˆé€šå¸¸ç­‰äºç‰¹å¾é‡‘å­—å¡”å±‚æ•°ï¼Œä¾‹ï¼š3ï¼‰
        self.reg_max = 16  # DFL çš„ç¦»æ•£ bins æ•°ï¼ˆæ¯ä¸ª bbox coordinate è¢«ç¦»æ•£åŒ–ä¸º reg_max ä¸ª binï¼‰ï¼Œç”¨äºæ›´ç»†ç²’åº¦çš„å›å½’è¡¨ç¤ºï¼ˆdistribution focal loss ç›¸å…³ï¼‰
        # no: æ¯ä¸ª anchor çš„è¾“å‡ºé€šé“æ•° = ç±»åˆ«æ•° + 4 ä¸ªåæ ‡ * reg_maxï¼ˆbox ä½¿ç”¨åˆ†å¸ƒå›å½’ï¼‰
        self.no = nc + self.reg_max * 4
        # stride æš‚æ—¶ç”¨ zeros å ä½ï¼Œä¼šåœ¨ build/make_anchors æ—¶è¢«æ›´æ–°
        self.stride = torch.zeros(self.nl)

        # è®¡ç®—ä¸­é—´é€šé“æ•°ä»¥ä¿æŒåˆç†å®¹é‡ï¼ˆç»éªŒå€¼ï¼‰
        # c2 ç”¨äº box åˆ†æ”¯çš„ä¸­é—´é€šé“ï¼›c3 ç”¨äºç±»åˆ«åˆ†æ”¯ä¸­é—´é€šé“
        c2 = max((16, ch[0] // 4, self.reg_max * 4))
        c3 = max(ch[0], min(self.nc, 100))

        # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»º box åˆ†æ”¯åºåˆ—ï¼šConv -> Conv -> Conv(out_channels=4*reg_max)
        # æœ€åä¸€å±‚ä¸å¸¦ activationï¼Œç›´æ¥è¾“å‡º raw logits / distribution logits
        self.cv2 = nn.ModuleList(
            nn.Sequential(Conv(x, c2, 3), Conv(c2, c2, 3), nn.Conv2d(c2, 4 * self.reg_max, 1)) for x in ch) # box åˆ†æ”¯ï¼Œæ¯å°ºåº¦æœ€å conv äº§ç”Ÿ 4*reg_max é€šé“

        # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»º cls åˆ†æ”¯åºåˆ—ï¼šConv -> Conv -> Conv(out_channels=nc)
        self.cv3 = nn.ModuleList(
            nn.Sequential(Conv(x, c3, 3), Conv(c3, c3, 3), nn.Conv2d(c3, self.nc, 1)) for x in ch) # ç±»åˆ«åˆ†æ”¯

        # å¦‚æœ reg_max>1 ä½¿ç”¨ DFL å°†ç¦»æ•£ logits è½¬ä¸ºåç§»å€¼ï¼ˆæœŸæœ›ï¼‰ï¼Œå¦åˆ™ Identity
        # DFL çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®©ç½‘ç»œé¢„æµ‹æ¯ä¸ªåæ ‡çš„ç¦»æ•£åˆ†å¸ƒï¼ˆæ¯”ç›´æ¥å›å½’æ›´ç¨³å®š/ç²¾ç»†ï¼‰
        self.dfl = DFL(self.reg_max) if self.reg_max > 1 else nn.Identity()

    def forward(self, x):
        """Concatenates and returns predicted bounding boxes and class probabilities.

        å‚æ•° x: list of feature mapsï¼Œé•¿åº¦ = self.nlï¼Œæ¯é¡¹å½¢çŠ¶ä¸º (B, C_i, H_i, W_i)
        è¿”å›ï¼š
        - training: è¿”å› listï¼Œæ¯ä¸ªå…ƒç´ ä¸ºè¯¥å°ºåº¦æ‹¼æ¥åçš„ logits tensor (B, no, H_i, W_i) ä¾¿äº loss è®¡ç®—
        - eval: è¿”å› (y, x) æˆ– yï¼Œy ä¸ºè§£ç åçš„é¢„æµ‹ (B, 4+nc, sum(H_i*W_i))ï¼ˆåŒ…å« bbox(xywh) + class_probï¼‰
        """

        shape = x[0].shape  # å–ç¬¬ä¸€ä¸ªå°ºåº¦çš„ BCHW ç”¨æ¥æ£€æµ‹ batch size å’Œå½¢çŠ¶å˜åŒ–
        # å¯¹æ¯ä¸ªå°ºåº¦ï¼Œå°† box åˆ†æ”¯å’Œ cls åˆ†æ”¯çš„è¾“å‡ºåœ¨ channel ç»´æ‹¼æ¥
        # box: (B, 4*reg_max, H, W), cls: (B, nc, H, W) -> concat => (B, no, H, W)
        for i in range(self.nl):
            x[i] = torch.cat((self.cv2[i](x[i]), self.cv3[i](x[i])), 1)

        # è®­ç»ƒæ¨¡å¼ç›´æ¥è¿”å›åŸå§‹ logitsï¼ˆloss è®¡ç®—éœ€è¦ raw logits/åˆ†å¸ƒï¼‰
        if self.training:
            return x
        # æ¨ç†æ¨¡å¼ï¼šå¦‚æœ dynamic æˆ– å½“å‰è¾“å…¥ shape å˜åŒ–ï¼Œåˆ™é‡æ–°ç”Ÿæˆ anchors å’Œ strides
        elif self.dynamic or self.shape != shape:
            # make_anchors æ ¹æ®å„å°ºåº¦ feature map å°ºå¯¸ä»¥åŠ strideï¼ˆå ä½ï¼‰æ„å»º anchor ç½‘æ ¼
            # è¿”å› anchors å’Œ stridesï¼Œæ¯ä¸ªæ˜¯åˆ—è¡¨ï¼ˆæŒ‰å°ºåº¦ï¼‰ã€‚YOLOv8å¯¹æ¯ä¸ª grid cell ç›´æ¥é¢„æµ‹ä¸€å¥—è¾“å‡ºï¼ˆè·ç¦»/åç§»ã€ç±»åˆ«ç­‰ï¼‰ï¼Œä½†æ˜¯è¦æŠŠè¿™äº›ç›¸å¯¹å€¼æ˜ å°„å›å›¾åƒåæ ‡ï¼Œä»ç„¶éœ€è¦ä¸€ä¸ªå‚è€ƒç‚¹ï¼ˆgrid centerï¼‰å’Œ strideï¼Œè¿™ä¸ªå‚è€ƒç‚¹åœ¨ä»£ç ä¸­å¸¸è¢«ç§°ä¸º anchors æˆ– grids
            # è¿™é‡Œä½¿ç”¨ç”Ÿæˆå™¨è¡¨è¾¾å¼è½¬ç½®å¼ é‡å½¢çŠ¶ä»¥åŒ¹é…åè¾¹ä½¿ç”¨çš„æ ¼å¼
            self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5)) # anchors = ç‰¹å¾å›¾ä¸Šæ¯ä¸ªæ ¼ç‚¹çš„å‚è€ƒåæ ‡ï¼ˆé€šå¸¸æ˜¯å½’ä¸€åŒ–çš„ä¸­å¿ƒåæ ‡ x, yï¼Œæœ‰æ—¶ä¹Ÿå¸¦ä¸€ä¸ªåŸºå‡† whï¼‰ï¼Œé…å¥—è¿˜æœ‰å¯¹åº”çš„ strideï¼ˆä¸‹é‡‡æ ·å€æ•°ï¼‰ã€‚å®ƒä»¬åªæ˜¯è§£ç ç½‘ç»œâ€œç›¸å¯¹é¢„æµ‹â€åˆ°å›¾åƒåæ ‡çš„å‚è€ƒç‚¹ï¼Œä¸æ˜¯é¢„è®¾çš„å¤šå°ºåº¦ã€å¤šçºµæ¨ªæ¯”é”šæ¡†
            self.shape = shape

        # æŠŠæ¯ä¸ªå°ºåº¦å±•å¹³å¹¶åœ¨ç©ºé—´ç»´åº¦ä¸Šæ‹¼æ¥ï¼ˆconcatenate across scalesï¼‰
        # xi.view(shape[0], self.no, -1) -> (B, no, H_i*W_i)
        x_cat = torch.cat([xi.view(shape[0], self.no, -1) for xi in x], 2)

        # export åˆ°æŸäº› TF ç›¸å…³æ ¼å¼æ—¶éœ€è¦é¿å…æŸäº›ç®—å­ï¼Œæ•…å¯¹ box/cls çš„åˆ‡åˆ†æ–¹å¼åšå…¼å®¹å¤„ç†
        if self.export and self.format in ('saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs'):  # avoid TF FlexSplitV ops
            box = x_cat[:, :self.reg_max * 4]
            cls = x_cat[:, self.reg_max * 4:]
        else:
            # æ ‡å‡†åˆ‡åˆ†ï¼šbox éƒ¨åˆ†å å‰ reg_max*4 ä¸ªé€šé“ï¼Œå…¶ä½™ä¸ºç±»åˆ« logits
            box, cls = x_cat.split((self.reg_max * 4, self.nc), 1)

        # å°† box çš„åˆ†å¸ƒ logits é€šè¿‡ DFL è½¬ä¸ºè¿ç»­å€¼åç§»ï¼ˆç›¸å¯¹äº anchor ç½‘æ ¼ï¼‰
        # dist2bboxï¼šæŠŠåç§»ä¸ anchors ç»“åˆå¹¶è¿”å› xywhï¼ˆæˆ–å…¶ä»–æ ¼å¼ï¼‰ï¼›
        # æ³¨æ„è¿™é‡Œå¯¹ anchors åšäº† unsqueeze(0) ä»¥åŒ¹é… batch ç»´åº¦
        dbox = dist2bbox(self.dfl(box), self.anchors.unsqueeze(0), xywh=True, dim=1) * self.strides
        # æœ€ç»ˆæ‹¼æ¥ï¼šbbox(xywh, å·²ä¹˜ stride => åƒç´ å°ºåº¦) + ç±»åˆ«æ¦‚ç‡(sigmoid)
        y = torch.cat((dbox, cls.sigmoid()), 1)
        # export æ—¶è¿”å› yï¼Œå¦åˆ™è¿”å› (y, x) å…¶ä¸­ x ä¸ºåŸå§‹ logits åˆ—è¡¨ï¼Œä¾¿äºåç»­ loss æˆ–è¿›ä¸€æ­¥å¤„ç†
        return y if self.export else (y, x)

    def bias_init(self):
        """Initialize Detect() biases, WARNING: requires stride availability.

        ç›®çš„ï¼šåˆå§‹åŒ–åç½®é¡¹ä½¿å¾—è®­ç»ƒåˆæœŸç±»åˆ«ç½®ä¿¡åº¦å’Œ bbox å›å½’ç¨³å®šã€‚
        - å¯¹ box åˆ†æ”¯åç½®è®¾ä¸º 1.0ï¼ˆç»éªŒå€¼ï¼Œä½¿ç½‘ç»œä¸€å¼€å§‹èƒ½é¢„æµ‹è¾ƒå¤§ bbox å€¼ï¼‰
        - å¯¹ cls åˆ†æ”¯åç½®è®¾ä¸ºä¸€ä¸ªä¸ stride ç›¸å…³çš„ logitï¼ˆä¼°è®¡å›¾ç‰‡ä¸­ç›®æ ‡æ¦‚ç‡ï¼‰
        æ³¨ï¼šéœ€è¦äº‹å…ˆçŸ¥é“æ¯å±‚çš„ strideï¼ˆé€šå¸¸åœ¨ build æˆ–ç¬¬ä¸€æ¬¡ forward æ—¶é€šè¿‡ make_anchors å¡«å…… self.strideï¼‰ã€‚
        """
        m = self  # ç®€å†™
        # ä»¥ä¸‹å¾ªç¯æŒ‰å°ºåº¦æ›´æ–°æœ€åä¸€å±‚ conv çš„ bias
        for a, b, s in zip(m.cv2, m.cv3, m.stride):  # a: box branch seq, b: cls branch seq, s: stride scalar
            a[-1].bias.data[:] = 1.0  # å°† box åˆ†æ”¯æœ€å conv çš„ bias è®¾ç½®ä¸º 1
            # ç±»åˆ«åç½®ï¼šä½¿ç”¨ç»éªŒå…¬å¼ log(5 / nc / (640 / s)^2)
            # ç›®çš„æ˜¯åŸºäº stride ä¸å›¾åƒå¤§å°ä¼°è®¡åˆå§‹ç‰©ä½“å¯†åº¦ï¼Œä½¿å¾—åˆå§‹ sigmoid æ¦‚ç‡æ¥è¿‘åˆç†å€¼
            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)  # cls (.01 objects, 80 classes, 640 img)


class Segment(Detect):
    """YOLOv8 Segment head for segmentation models.

    æ‰©å±• Detectï¼š
    - å¢åŠ  Proto ç½‘ç»œç”ŸæˆåŸå‹ masksï¼ˆpï¼‰ï¼Œé€šå¸¸åˆ†è¾¨ç‡è¾ƒé«˜ï¼›proto æä¾›åŸºåº• mask å›¾ï¼ˆé€šå¸¸é€šé“æ•° nprï¼Œæ¯ä¸ªé€šé“æ˜¯ä¸€å¼ åŸºç¡€ maskï¼‰ï¼Œæ¯ä¸ªå®ä¾‹çš„æœ€ç»ˆ mask é€šè¿‡ mask coefficients ä¸ proto åšçº¿æ€§ç»„åˆï¼ˆåæ¥ sigmoid/thresholdï¼‰å¾—åˆ°
    - å¢åŠ æ¯ä¸ªé¢„æµ‹çš„ mask coefficientsï¼ˆmcï¼‰ï¼Œé€šè¿‡çº¿æ€§ç»„åˆ proto å¾—åˆ°å®ä¾‹ maskã€‚
    - è¿™ç§è®¾è®¡æŠŠ mask çš„åˆ†è¾¨ç‡å¼€é”€ä»æ¯ä¸ªé¢„æµ‹çš„ç‹¬ç«‹è¾“å‡ºé™åˆ°å…±äº« proto + ä½ç»´ coeffsï¼Œæ›´èŠ‚çœè®¡ç®—ä¸å‚æ•°
    è®¾è®¡äº®ç‚¹ï¼šé€šè¿‡å…±äº« protoï¼ˆé«˜åˆ†è¾¨ç‡ç‰¹å¾çš„å¤šä¸ªé€šé“ï¼‰+ æ¯å®ä¾‹ä½ç»´ç³»æ•°ï¼Œé¿å…ä¸ºæ¯ä¸ªå®ä¾‹è¾“å‡ºå®Œæ•´åˆ†è¾¨ç‡ maskï¼ŒèŠ‚çœè®¡ç®—ä¸å†…å­˜ã€‚
    """

    def __init__(self, nc=80, nm=32, npr=256, ch=()):
        """Initialize the YOLO model attributes such as the number of masks, prototypes, and the convolution layers."""
        super().__init__(nc, ch)
        self.nm = nm  # mask æ•°é‡ï¼ˆmask coefficients çš„æ•°é‡ï¼‰
        self.npr = npr  # proto çš„é€šé“æ•°ï¼ˆå³ proto åŸºåº•æ•°é‡ï¼‰
        # Proto ç½‘ç»œï¼šä»ç¬¬ä¸€ä¸ªå°ºåº¦çš„ç‰¹å¾ç”Ÿæˆ proto åŸå‹ masksï¼Œè¾“å‡ºå½¢çŠ¶ (B, npr, H_p, W_p)
        self.proto = Proto(ch[0], self.npr, self.nm)  # proto ç½‘ç»œï¼Œé€šå¸¸ä»é«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼ˆç¬¬ä¸€ä¸ªå°ºåº¦ï¼‰ç”Ÿæˆ Prototype maskï¼ˆpï¼‰
        # ä¿å­˜ Detect.forward çš„æ–¹æ³•å¼•ç”¨ï¼Œæ–¹ä¾¿å¤ç”¨çˆ¶ç±»çš„é¢„æµ‹é€»è¾‘ï¼ˆä¸è¦†ç›–çˆ¶ç±» forwardï¼‰
        self.detect = Detect.forward

        # cv4 ç”¨æ¥äº§ç”Ÿ mask coefficientsï¼ˆæ¯ä¸ªå°ºåº¦ä¸€å¥—åˆ†æ”¯ï¼‰
        # c4 æ˜¯åˆ†æ”¯çš„ä¸­é—´é€šé“æ•°ï¼Œè‡³å°‘ä¸º nm
        c4 = max(ch[0] // 4, self.nm) # æ¯ä¸ªå°ºåº¦ç”Ÿæˆ mask coefficients çš„å·ç§¯åˆ†æ”¯ï¼ˆæœ€åè¾“å‡º nm é€šé“ï¼‰ï¼Œç”¨äºç»™æ¯ä¸ªé¢„æµ‹ç”Ÿæˆå¯¹åº”çš„ mask coefficientsï¼ˆçº¿æ€§ç»„åˆ protoï¼‰
        # æ¯ä¸ªå°ºåº¦è¾“å‡ºå±‚ä¸º Conv(...)->Conv(...)->Conv(out_channels=nm)
        self.cv4 = nn.ModuleList(
            nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)

    def forward(self, x):
        """Return model outputs and mask coefficients if training, otherwise return outputs and mask coefficients."""
        # 1) ç”Ÿæˆ proto åŸå‹ masksï¼ˆé«˜åˆ†è¾¨ç‡ï¼‰ï¼Œå½¢çŠ¶ p: (B, npr, Hp, Wp)
        p = self.proto(x[0])  # mask protosï¼Œä»ç¬¬ä¸€ä¸ªå°ºåº¦ç”Ÿæˆ protoï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªè¾ƒé«˜åˆ†è¾¨ç‡çš„ç‰¹å¾å›¾ï¼Œæœ€ç»ˆç”¨çº¿æ€§ç»„åˆç”Ÿæˆ masksï¼‰
        bs = p.shape[0]  # batch size

        # 2) mask coefficients: å¯¹æ¯ä¸ªå°ºåº¦çš„ç‰¹å¾è·‘ cv4 åˆ†æ”¯ï¼Œç„¶å view åˆ° (B, nm, H_i*W_i)ï¼Œåœ¨å°ºåº¦ç»´åº¦ concat => (B, nm, sum(H_i*W_i))
        mc = torch.cat([self.cv4[i](x[i]).view(bs, self.nm, -1) for i in range(self.nl)], 2)  # mask coefficientsï¼Œreshapeä¸º (bs, nm, -1) æŠŠæ¯ä¸ªé¢„æµ‹çš„ mask coefficients æŒ‰ç©ºé—´æ‹¼æ¥

        # 3) ä½¿ç”¨ detect çš„ forward å¾—åˆ°æ£€æµ‹è¾“å‡ºï¼ˆåŸå§‹ logits æˆ–è§£ç åçš„ yï¼‰
        x = self.detect(self, x) # è°ƒç”¨ Detect.forward æ¥è·å–æ£€æµ‹éƒ¨åˆ†è¾“å‡º
        if self.training:
            # è®­ç»ƒæ—¶è¿”å›ï¼šæ£€æµ‹è¾“å‡º xï¼ˆlogits åˆ—è¡¨ï¼‰ã€mask coefficients mcã€proto p
            # loss ç«¯ä¼šä½¿ç”¨è¿™äº›é‡è®¡ç®— mask losses ç­‰
            return x, mc, p
        # æ¨ç†/å¯¼å‡ºæ—¶ï¼Œè¿”å›æ ¼å¼ä¸åŒä»¥é…åˆåç»­æ¨ç†æµæ°´çº¿ï¼š
        # - export æ—¶é€šå¸¸è¦æŠŠ mc å¹¶åˆ°æ£€æµ‹è¾“å‡ºé€šé“ç»´ï¼Œä¾¿äºå¯¼å‡ºä¸ºå•å¼  tensor
        # - é export æ—¶è¿”å› (concatenated, (x[1], mc, p)) å…¶ä¸­ x[1] æ˜¯åŸå§‹ logits åˆ—è¡¨
        return (torch.cat([x, mc], 1), p) if self.export else (torch.cat([x[0], mc], 1), (x[1], mc, p))


class SegmentPose(Detect):
    """YOLOv8 SegmentPose head for segmentation-pose models.
    å…³ç³»ï¼šSegmentPose æ˜¯å¯¹ Detect çš„æ‰©å±•ï¼Œå…¼é¡¾ segmentation å’Œ pose ä¸¤ä¸ªä»»åŠ¡ï¼›ç›¸å½“äºåœ¨åŒä¸€ä¸ªæ£€æµ‹ç»“æ„ä¸Šå¹¶è¡Œå¢åŠ ä¸¤ç±»é¢å¤–è¾“å‡º branchï¼ˆmask coeff ä¸ keypoint logitsï¼‰ï¼Œä½¿æ¨¡å‹å¯åœ¨å•æ¬¡å‰å‘é‡ŒåŒæ—¶é¢„æµ‹ bbox/classã€maskã€keypoint

    ç»¼åˆ Segment ä¸ Poseï¼š
    - é›†æˆäº† Segment çš„ proto + mask coefficient åˆ†æ”¯ï¼ˆcv4 + protoï¼‰å’Œ Pose çš„å…³é”®ç‚¹åˆ†æ”¯ï¼ˆcv5ï¼‰
    - cv4ï¼šè¾“å‡º nm ä¸ª mask coefficientsï¼ˆå’Œ Segment ä¸€æ ·ï¼‰ï¼›cv5ï¼šè¾“å‡º nk ä¸ª keypoint é€šé“ï¼ˆå’Œ Pose çš„ cv4 ç±»ä¼¼ï¼Œä½†æ”¾åœ¨ cv5ï¼‰
    - å…·æœ‰ proto + mask coefficients åˆ†æ”¯ï¼ˆç”¨äºå®ä¾‹åˆ†å‰²ï¼‰
    - åŒæ—¶å…·æœ‰ keypoint åˆ†æ”¯ï¼ˆç”¨äºäººä½“/ç‰©ä½“å…³é”®ç‚¹ï¼‰
    - è®­ç»ƒæ—¶è¿”å›åŸå§‹ logits + mc + p + kptï¼ˆkpt ä¸ºåŸå§‹æœªè§£ç çš„ keypoint predictionsï¼‰
    - æ¨ç†æ—¶å¯¹ keypoints åšè§£ç ï¼Œè¿”å› decoded keypoints ä»¥ä¾¿åç»­å¯è§†åŒ–æˆ– NMS åˆå¹¶
    è®¾è®¡åŠ¨æœºï¼šåœ¨ä¸€æ¬¡å‰å‘ä¸­åŒæ—¶é¢„æµ‹ bbox/classã€maskã€keypointsï¼Œå…±äº« backbone/neck ç‰¹å¾ï¼ŒèŠ‚çœè®¡ç®—å¹¶ä¿è¯ä»»åŠ¡è¾“å‡ºå¯¹é½ï¼ˆanchor/grid å¯¹é½ï¼‰ã€‚
    """

    def __init__(self, nc=80, kpt_shape=(17, 3), nm=32, npr=256, ch=()):
        """Initialize the YOLO model attributes such as the number of masks, prototypes, and the convolution layers."""
        super().__init__(nc, ch)
        # Segment éƒ¨åˆ†å‚æ•°
        self.nm = nm  # mask coefficients æ•°é‡
        self.npr = npr  # proto é€šé“æ•°
        self.proto = Proto(ch[0], self.npr, self.nm)  # proto ç½‘ç»œ
        self.detect = Detect.forward

        c4 = max(ch[0] // 4, self.nm)
        self.cv4 = nn.ModuleList(
            nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nm, 1)) for x in ch)

        # Pose éƒ¨åˆ†å‚æ•°
        self.kpt_shape = kpt_shape  # (num_kpts, dims) dims é€šå¸¸ä¸º 2(x,y) æˆ– 3(x,y,visibility/score)
        self.nk = kpt_shape[0] * kpt_shape[1]  # æ€»è¾“å‡ºé€šé“æ•°ï¼ˆæ¯ä¸ª keypoint æœ‰ ndim ä¸ªé€šé“ï¼‰
        c5 = max(ch[0] // 4, self.nk)
        # cv5 ä¸º keypoint åˆ†æ”¯ï¼Œä¸ºæ¯ä¸ªå°ºåº¦äº§ç”Ÿ nk é€šé“çš„ map
        self.cv5 = nn.ModuleList(
            nn.Sequential(Conv(x, c5, 3), Conv(c5, c5, 3), nn.Conv2d(c5, self.nk, 1)) for x in ch)

    def forward(self, x):
        """Return model outputs and mask coefficients if training, otherwise return outputs and mask coefficients."""
        # 1) proto
        p = self.proto(x[0])  # mask protos (B, npr, Hp, Wp)
        bs = p.shape[0]  # batch size

        # 2) mask coefficientsï¼ŒåŒ Segmentï¼š(B, nm, sum(H_i*W_i))
        mc = torch.cat([self.cv4[i](x[i]).view(bs, self.nm, -1) for i in range(self.nl)], 2)  # mask coefficients

        # 3) keypointsï¼šå¯¹æ¯ä¸ªå°ºåº¦å– cv5 è¾“å‡º -> reshape concat -> (B, nk, sum(H_i*W_i))
        kpt = torch.cat([self.cv5[i](x[i]).view(bs, self.nk, -1) for i in range(self.nl)], -1)  # (bs, nk, h*w)

        # 4) è°ƒç”¨ Detect çš„ forward å¾—åˆ°æ£€æµ‹è¾“å‡º
        x = self.detect(self, x)
        if self.training:
            # è®­ç»ƒï¼šè¿”å›åŸå§‹æ£€æµ‹ logits åˆ—è¡¨ xã€mcã€proto pã€ä»¥åŠåŸå§‹ keypoint logits kptï¼ˆåç»­ loss ç”¨ï¼‰
            return x, mc, p, kpt

        # æ¨ç†ï¼šéœ€è¦å¯¹ keypoint åšè§£ç ï¼ˆä»ç½‘æ ¼/anchor ç›¸å¯¹å€¼ -> å›¾åƒå°ºåº¦åæ ‡ï¼‰
        pred_kpt = self.kpts_decode(bs, kpt)
        # æ¨ç†æ—¶è¾“å‡ºæ ¼å¼ï¼š
        # - export: æŠŠ x å’Œ mc ä»¥åŠ pred_kpt åœ¨ channel ç»´æ‹¼æ¥ï¼Œç„¶åè¿”å› proto pï¼ˆå¯¼å‡ºéœ€æ±‚ï¼‰
        # - æ­£å¸¸æ¨ç†: è¿”å› (concat(x[0], mc, pred_kpt), (x[1], mc, p, kpt))ï¼Œå…¶ä¸­ x[1] ä¸ºåŸå§‹ logits åˆ—è¡¨
        return (torch.cat([x, mc, pred_kpt], 1), p) if self.export else (torch.cat([x[0], mc, pred_kpt], 1), (x[1], mc, p, kpt))
    
    def kpts_decode(self, bs, kpts):
        """Decodes keypoints from raw head output to image-scale coordinates.

        è¾“å…¥ kpts: (B, nk, N) å…¶ä¸­ nk = num_kpts * ndim, N = sum(H_i*W_i)
        è¿”å›: ä¸ prediction å¯¹é½çš„ decoded keypointsï¼Œä»ç„¶ä»¥é€šé“ä¼˜å…ˆ (B, nk, N)

        è§£ç é€»è¾‘è¦ç‚¹ï¼š
        - å½“ ndim == 3 æ—¶ï¼Œç¬¬ 3 ä¸ªé€šé“é€šå¸¸è¡¨ç¤ºå¯è§æ€§/ç½®ä¿¡åº¦ -> å¯¹è¯¥é€šé“åš sigmoidã€‚
        - å¯¹ x,y é€šé“æŒ‰ç½‘æ ¼ anchors ä¸ strides åšçº¿æ€§å˜æ¢ï¼š
            y_x = (raw * 2.0 + (anchor_coord - 0.5)) * stride
          è¿™é‡Œ raw * 2.0 æ‰©å±•äº†ç½‘ç»œè¾“å‡ºèŒƒå›´ï¼ˆå¸¸è§æŠ€å·§ï¼Œé…åˆ sigmoid/åŸå§‹å€¼ä½¿ç”¨ï¼‰ï¼Œanchor_coord æ˜¯ç½‘æ ¼åæ ‡ï¼ˆ0..W-1 / H-1ï¼‰
          anchor_coord - 0.5 è°ƒæ•´ä¸­å¿ƒåç§»ï¼Œä½¿é¢„æµ‹ç›¸å¯¹äºåƒç´ åæ ‡æ­£ç¡®ã€‚
        - self.anchors, self.strides æ˜¯åœ¨ Detect.forward ä¸­ç”± make_anchors å¡«å……çš„ï¼Œå…¨å±€å…±äº«ã€‚
        """
        ndim = self.kpt_shape[1]
        # ç›´æ¥å¤åˆ¶ tensor é¿å…è¦†ç›–åŸå§‹ kptsï¼ˆåç»­è®­ç»ƒæ—¶ä»éœ€åŸå§‹ kptï¼‰
        y = kpts.clone()
        if ndim == 3:
            # å¯¹æ¯ä¸ªå…³é”®ç‚¹çš„ visibility/score é€šé“åš inplace sigmoidï¼Œç´¢å¼•ä¸º 2::3ï¼ˆå‡è®¾ ndim==3ï¼‰
            y[:, 2::3].sigmoid_()  # inplace sigmoid
        # x é€šé“ç´¢å¼•ä¸º 0::ndimï¼Œy é€šé“ç´¢å¼•ä¸º 1::ndim
        # anchors å’Œ strides çš„å½¢çŠ¶ä¸ (2, N) æˆ– (N,) ç­‰æœ‰å…³ï¼Œå…·ä½“åœ¨ make_anchors ä¸­ç”Ÿæˆ
        # å°†ç½‘ç»œè¾“å‡ºå˜æ¢åˆ°å›¾åƒå°ºåº¦
        y[:, 0::ndim] = (y[:, 0::ndim] * 2.0 + (self.anchors[0] - 0.5)) * self.strides
        y[:, 1::ndim] = (y[:, 1::ndim] * 2.0 + (self.anchors[1] - 0.5)) * self.strides
        return y


class Pose(Detect):
    """YOLOv8 Pose head for keypoints models.

    åœ¨ Detect åŸºç¡€ä¸Šå¢åŠ å…³é”®ç‚¹åˆ†æ”¯ cv4ï¼Œå¹¶å®ç° kpts è§£ç ã€‚ä¸ SegmentPose åŒºåˆ«åœ¨äº Pose åªè´Ÿè´£ keypointsï¼ˆä¸åŒ…å« mask/protoï¼‰ã€‚
    """

    def __init__(self, nc=80, kpt_shape=(17, 3), ch=()):
        """Initialize YOLO network with default parameters and Convolutional Layers."""
        super().__init__(nc, ch)
        self.kpt_shape = kpt_shape  # (num_kpts, ndim)
        self.nk = kpt_shape[0] * kpt_shape[1]  # è¾“å‡ºé€šé“æ•°ï¼Œæœ€ç»ˆæ¯å°ºåº¦æœ€å conv è¾“å‡º nk é€šé“
        self.detect = Detect.forward  # å¤ç”¨çˆ¶ç±» Detect çš„æ£€æµ‹é€»è¾‘ï¼ˆbbox + cls éƒ¨åˆ†ï¼‰

        # ä¸­é—´é€šé“æ•° c4 è‡³å°‘ä¸º nk
        c4 = max(ch[0] // 4, self.nk)
        # cv4 ä¸ºæ¯ä¸ªå°ºåº¦çš„ keypoint åˆ†æ”¯ï¼šConv -> Conv -> Conv(out_channels=nk)
        self.cv4 = nn.ModuleList(
            nn.Sequential(Conv(x, c4, 3), Conv(c4, c4, 3), nn.Conv2d(c4, self.nk, 1)) for x in ch)

    def forward(self, x):
        """Perform forward pass through YOLO model and return predictions.

        x: list of feature maps
        è¿”å›ï¼š
        - training: (x, kpt) å…¶ä¸­ x ä¸º detect çš„ logits åˆ—è¡¨ï¼Œkpt ä¸ºåŸå§‹ keypoint logits
        - eval: è¿”å› (concatenated preds, (x[1], kpt)) æˆ–å¯¼å‡ºæ—¶ self.export å…¼å®¹æ ¼å¼
        """
        bs = x[0].shape[0]  # batch size
        # å°†æ¯ä¸ªå°ºåº¦ cv4 è¾“å‡º reshape å concat -> (bs, nk, sum(H_i*W_i))
        kpt = torch.cat([self.cv4[i](x[i]).view(bs, self.nk, -1) for i in range(self.nl)], -1)  # (bs, nk, h*w)
        # è°ƒç”¨ Detect.forward å¾—åˆ°æ£€æµ‹è¾“å‡º
        x = self.detect(self, x)
        if self.training:
            # è®­ç»ƒï¼šè¿”å› detect logits åˆ—è¡¨ ä¸ keypoint åŸå§‹ logitsï¼ˆç”¨äº lossï¼‰
            return x, kpt
        # æ¨ç†æ—¶å¯¹ keypoint åšè§£ç ï¼ˆå¾—åˆ°å›¾åƒå°ºåº¦åæ ‡ï¼‰
        pred_kpt = self.kpts_decode(bs, kpt)
        # export æ—¶è¿”å›ä¸åŒæ ¼å¼ä»¥å…¼å®¹å¯¼å‡ºå·¥å…·ï¼Œå¦åˆ™è¿”å› (concat(x[0], pred_kpt), (x[1], kpt))
        return torch.cat([x, pred_kpt], 1) if self.export else (torch.cat([x[0], pred_kpt], 1), (x[1], kpt))

    def kpts_decode(self, bs, kpts):
        """Decodes keypoints.

        ä¸ SegmentPose.kpts_decode ç›¸ä¼¼ï¼Œä½†åœ¨ export è·¯å¾„ä¸­ shape å¤„ç†ç•¥æœ‰ä¸åŒä»¥è§„é¿å¯¼å‡º bugã€‚
        """
        ndim = self.kpt_shape[1]
        if self.export:  # required for TFLite export to avoid 'PLACEHOLDER_FOR_GREATER_OP_CODES' bug
            # export æƒ…å†µä¸‹ï¼Œå°†é€šé“é‡ç»„ä¸º (bs, num_kpts, ndim, N)
            y = kpts.view(bs, *self.kpt_shape, -1)
            # è®¡ç®— x,y: (y[:, :, :2] * 2.0 + (anchors - 0.5)) * strides
            # æ³¨æ„æ­¤å¤„ self.anchors çš„å½¢çŠ¶æ˜¯é’ˆå¯¹ keypoint è§£ç  export ç‰¹æ®Šè·¯å¾„çš„é¢„æœŸæ ¼å¼
            a = (y[:, :, :2] * 2.0 + (self.anchors - 0.5)) * self.strides
            # è‹¥æœ‰å¯è§æ€§é€šé“åˆ™æ‹¼æ¥ sigmoid åçš„å¯è§æ€§
            if ndim == 3:
                a = torch.cat((a, y[:, :, 2:3].sigmoid()), 2)
            # é‡æ–° view ä¸º (bs, nk, N)
            return a.view(bs, self.nk, -1)
        else:
            # éå¯¼å‡ºè·¯å¾„ï¼šä½¿ç”¨ inplace sigmoid + åŸºäº anchors/strides çš„çº¿æ€§æ˜ å°„ï¼ˆä¸ SegmentPose ä¸€è‡´ï¼‰
            y = kpts.clone()
            if ndim == 3:
                y[:, 2::3].sigmoid_()  # inplace sigmoid for visibility
            y[:, 0::ndim] = (y[:, 0::ndim] * 2.0 + (self.anchors[0] - 0.5)) * self.strides
            y[:, 1::ndim] = (y[:, 1::ndim] * 2.0 + (self.anchors[1] - 0.5)) * self.strides
            return y


class Classify(nn.Module):
    """YOLOv8 classification head, i.e. x(b,c1,20,20) to x(b,c2).

    ç®€å•çš„åˆ†ç±»å¤´ï¼šå…ˆ conv -> å…¨å±€æ± åŒ– -> dropout -> linear è¾“å‡ºç±»åˆ«æ¦‚ç‡ï¼ˆè®­ç»ƒè¿”å› logitsï¼Œæ¨ç†è¿”å› softmaxï¼‰ã€‚
    å¸¸ç”¨äº image-level åˆ†ç±»è€Œéæ£€æµ‹åˆ†æ”¯ã€‚
    """

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__()
        c_ = 1280  # æŒ‰ EfficientNet-B0 çš„ä¸­é—´é€šé“è§„æ¨¡é€‰æ‹©
        self.conv = Conv(c1, c_, k, s, p, g)
        self.pool = nn.AdaptiveAvgPool2d(1)  # å…¨å±€æ± åŒ–åˆ° (B, c_, 1, 1)
        self.drop = nn.Dropout(p=0.0, inplace=True)
        self.linear = nn.Linear(c_, c2)  # fc -> c2

    def forward(self, x):
        """Performs a forward pass of the YOLO model on input image data."""
        if isinstance(x, list):
            # è‹¥ä¼ å…¥æ˜¯ listï¼ˆå¤šå°ºåº¦ç‰¹å¾ï¼‰ï¼Œåˆ™åœ¨ channel ç»´åº¦æ‹¼æ¥ï¼ˆéå¸¸å°‘è§çš„ç”¨æ³•ï¼‰
            x = torch.cat(x, 1)
        # conv -> pool -> flatten -> dropout -> linear
        x = self.linear(self.drop(self.pool(self.conv(x)).flatten(1)))
        # è®­ç»ƒè¿”å› logitsï¼ˆä¾› lossï¼‰ï¼Œæ¨ç†è¿”å› softmax æ¦‚ç‡åˆ†å¸ƒ
        return x if self.training else x.softmax(1)


class RTDETRDecoder(nn.Module):
    """åŸºäº Deformable Transformer çš„ç®€åŒ– DETR è§£ç å™¨æ¨¡å—ï¼ˆquery-based detectionï¼‰ã€‚

    è¯¥ç±»ä¸ä¸Šé¢çš„ YOLO-style heads ä¸åŒï¼Œå±äº query-based æ£€æµ‹å™¨çš„ decoder å®ç°ç¤ºä¾‹ã€‚
    ä¸»è¦åŒ…å«ï¼š
    - ç‰¹å¾æŠ•å½± input_projï¼Œå°† backbone å¤šå°ºåº¦ç‰¹å¾æŠ•å½±åˆ°ç›¸åŒ hidden dim
    - Deformable Transformer decoderï¼ˆä½¿ç”¨ DeformableTransformerDecoderLayerï¼‰
    - Encoder-side headï¼ˆç»™ encoder è¾“å‡ºåšåˆæ­¥å€™é€‰å¹¶é€‰ topk ä½œä¸º queriesï¼‰
    - Denoising training æ”¯æŒï¼ˆquery å‰æ·»åŠ å™ªå£°æ ·ä¾‹ä»¥ç¨³å®šè®­ç»ƒï¼‰
    æ³¨æ„ï¼šæ­¤å®ç°ä¸ºç®€åŒ–ç‰ˆæœ¬ï¼Œä¸å®Œå…¨ä¸å®˜æ–¹ checkpoint æƒé‡ç›´æ¥å¯¹åº”ï¼Œä½†ç”¨äºå±•ç¤º QUERY è§£ç å™¨é€»è¾‘ã€‚
    """
    export = False  # export mode

    def __init__(
            self,
            nc=80,
            ch=(512, 1024, 2048),
            hd=256,  # hidden dim
            nq=300,  # num queries
            ndp=4,  # num decoder points
            nh=8,  # num head
            ndl=6,  # num decoder layers
            d_ffn=1024,  # dim of feedforward
            dropout=0.,
            act=nn.ReLU(),
            eval_idx=-1,
            # training args
            nd=100,  # num denoising
            label_noise_ratio=0.5,
            box_noise_scale=1.0,
            learnt_init_query=False):
        super().__init__()
        self.hidden_dim = hd
        self.nhead = nh
        self.nl = len(ch)  # num levels (backbone feature levels)
        self.nc = nc
        self.num_queries = nq
        self.num_decoder_layers = ndl

        # backbone feature projection -> å°†æ¯ä¸ªå°ºåº¦çš„é€šé“æŠ•å½±åˆ°ç›¸åŒ hidden dimï¼ˆä¾¿äº transformer å¤„ç†ï¼‰
        self.input_proj = nn.ModuleList(
            nn.Sequential(nn.Conv2d(x, hd, 1, bias=False), nn.BatchNorm2d(hd)) for x in ch)
        # NOTE: æ³¨é‡Šä¸­æåˆ°çš„ Conv ç‰ˆæœ¬è¢«ç®€åŒ–ä¸ºä¸Šè¿°å®ç°ä»¥å…¼å®¹éƒ¨åˆ†æƒé‡

        # Transformer decoder å±‚ä¸æ•´ä½“ decoderï¼ˆdeformable attentionï¼‰
        decoder_layer = DeformableTransformerDecoderLayer(hd, nh, d_ffn, dropout, act, self.nl, ndp)
        self.decoder = DeformableTransformerDecoder(hd, decoder_layer, ndl, eval_idx)

        # denoising embeddingï¼Œç”¨äº denoising training æ¡†æ¶ï¼ˆCDNï¼‰
        self.denoising_class_embed = nn.Embedding(nc, hd)
        self.num_denoising = nd
        self.label_noise_ratio = label_noise_ratio
        self.box_noise_scale = box_noise_scale

        # decoder query åˆå§‹åŒ–ï¼ˆå¯ä»¥é€‰æ‹©å­¦ä¹ å‹æˆ–åŠ¨æ€ä» encoder è¾“å‡ºé€‰å–ï¼‰
        self.learnt_init_query = learnt_init_query
        if learnt_init_query:
            self.tgt_embed = nn.Embedding(nq, hd)
        # æ ¹æ® bbox (4-dim) ç”Ÿæˆ query position embedding çš„ headï¼ˆMLPï¼‰
        self.query_pos_head = MLP(4, 2 * hd, hd, num_layers=2)

        # encoder ä¾§ headï¼šç”¨äºä» encoder è¾“å‡ºä¸­é€‰å– top-K å€™é€‰ query
        self.enc_output = nn.Sequential(nn.Linear(hd, hd), nn.LayerNorm(hd))
        self.enc_score_head = nn.Linear(hd, nc)
        self.enc_bbox_head = MLP(hd, hd, 4, num_layers=3)

        # decoder ä¾§ headï¼šæ¯å±‚ decoder éƒ½æœ‰ç‹¬ç«‹çš„åˆ†ç±»/å›å½’ headï¼ˆç”¨äº deep supervisionï¼‰
        self.dec_score_head = nn.ModuleList([nn.Linear(hd, nc) for _ in range(ndl)])
        self.dec_bbox_head = nn.ModuleList([MLP(hd, hd, 4, num_layers=3) for _ in range(ndl)])

        self._reset_parameters()

    def forward(self, x, batch=None):
        from ultralytics.vit.utils.ops import get_cdn_group

        # 1) input projection + embedding
        feats, shapes = self._get_encoder_input(x)

        # 2) denoising training å‡†å¤‡ï¼ˆå¦‚æœè®­ç»ƒä¸”ä½¿ç”¨ CDNï¼‰
        dn_embed, dn_bbox, attn_mask, dn_meta = \
            get_cdn_group(batch,
                          self.nc,
                          self.num_queries,
                          self.denoising_class_embed.weight,
                          self.num_denoising,
                          self.label_noise_ratio,
                          self.box_noise_scale,
                          self.training)

        # 3) è·å– decoder inputï¼ˆembeddings, reference bboxes ç­‰ï¼‰
        embed, refer_bbox, enc_bboxes, enc_scores = \
            self._get_decoder_input(feats, shapes, dn_embed, dn_bbox)

        # 4) decoder forward
        dec_bboxes, dec_scores = self.decoder(embed,
                                              refer_bbox,
                                              feats,
                                              shapes,
                                              self.dec_bbox_head,
                                              self.dec_score_head,
                                              self.query_pos_head,
                                              attn_mask=attn_mask)
        # è¿”å›ç»“æ„ï¼šdecoder è¾“å‡ºçš„ bbox/score å’Œ encoder-side çš„ bbox/score ä»¥åŠ denoising å…ƒä¿¡æ¯
        x = dec_bboxes, dec_scores, enc_bboxes, enc_scores, dn_meta
        if self.training:
            return x
        # æ¨ç†ï¼šæ‹¼æ¥ decoder æœ€åä¸€å±‚ bbox ä¸ sigmoid åçš„ score
        # (bs, 300, 4+nc)
        y = torch.cat((dec_bboxes.squeeze(0), dec_scores.squeeze(0).sigmoid()), -1)
        return y if self.export else (y, x)

    def _generate_anchors(self, shapes, grid_size=0.05, dtype=torch.float32, device='cpu', eps=1e-2):
        """ä¸º Deformable DETR é£æ ¼ç”Ÿæˆ anchorsï¼ˆå‚è€ƒæ¡†ï¼‰ï¼Œè¿”å› anchorsï¼ˆlogit ç©ºé—´ï¼‰å’Œ valid_mask.
        åŸºäº Deformable Transformer çš„æ£€æµ‹è§£ç å™¨ï¼ˆä¸ YOLO ç³»åˆ—ä¸åŒé£æ ¼ï¼‰ï¼ŒåŒ…å« encoder->decoder æµç¨‹ã€anchor ç”Ÿæˆã€denoising training æœºåˆ¶ã€decoder headsã€‚æ–‡ä»¶ä¸­æ­¤ç±»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ transformer decoder head çš„ç®€åŒ–å®ç°ï¼Œé YOLO çš„ä¸»å¹²éƒ¨åˆ†ï¼Œä½†æ”¾åœ¨åŒæ–‡ä»¶ä»¥ä¾¿å¤ç”¨/æ›¿ä»£åˆ†æã€‚ä¸»è¦ç”¨äº query-based æ£€æµ‹ï¼ˆå¦‚ DETR å˜ä½“ï¼‰çš„ä»»åŠ¡

        shapes: list of [h, w] æ¯ä¸ªå°ºåº¦çš„ç©ºé—´å°ºå¯¸
        grid_size: åŸºç¡€å°ºåº¦ï¼ˆanchor çš„ whï¼‰
        ä¸»è¦æµç¨‹ï¼š
        - ç”Ÿæˆç½‘æ ¼åæ ‡ grid_xyï¼ˆ0..w-1, 0..h-1ï¼‰ï¼Œå½’ä¸€åŒ–åˆ° (0,1) å¹¶åŠ  0.5 åç§»ä½¿ä¸­å¿ƒå¯¹é½
        - wh æ ¹æ®å°ºåº¦ä»¥å€æ•°å¢é•¿
        - anchors shape (1, h*w*nl, 4)ï¼Œæœ€åå¯¹æ•°åŒ– (logit ç©ºé—´) ä»¥ä¾¿åç»­ä¸ç½‘ç»œè¾“å‡ºç›¸åŠ 
        - valid_mask: æ ‡è®°é‚£äº› anchors åœ¨ (eps, 1-eps) åŒºé—´å†…çš„åˆæ³•æ€§ï¼ˆé¿å…è¾¹ç•Œå€¼ï¼‰
        """
        anchors = []
        for i, (h, w) in enumerate(shapes):
            # meshgrid äº§ç”Ÿ y,x ç½‘æ ¼
            grid_y, grid_x = torch.meshgrid(torch.arange(end=h, dtype=dtype, device=device),
                                            torch.arange(end=w, dtype=dtype, device=device),
                                            indexing='ij')
            grid_xy = torch.stack([grid_x, grid_y], -1)  # (h, w, 2) æŒ‰ (x, y)

            valid_WH = torch.tensor([h, w], dtype=dtype, device=device)
            # å½’ä¸€åŒ–ä¸º (0..1)ï¼Œå¹¶åŠ ä¸Š 0.5/valid_WH ç”¨äºä¸­å¿ƒå¯¹é½
            grid_xy = (grid_xy.unsqueeze(0) + 0.5) / valid_WH  # (1, h, w, 2)
            # wh æŒ‰å°ºåº¦ç¼©æ”¾ï¼ŒåŸºç¡€ä¸º grid_size * (2^i)
            wh = torch.ones_like(grid_xy, dtype=dtype, device=device) * grid_size * (2.0 ** i)
            anchors.append(torch.cat([grid_xy, wh], -1).view(-1, h * w, 4))  # (1, h*w, 4)

        anchors = torch.cat(anchors, 1)  # (1, h*w*nl, 4)
        # valid mask: å¦‚æœ anchors çš„æ‰€æœ‰ç»´åº¦éƒ½åœ¨ (eps, 1-eps) å†…åˆ™ä¸ºæœ‰æ•ˆ
        valid_mask = ((anchors > eps) * (anchors < 1 - eps)).all(-1, keepdim=True)  # 1, h*w*nl, 1
        # å°† anchors è½¬ä¸º logit ç©ºé—´ï¼ˆlog(x/(1-x)))ï¼Œæ–¹ä¾¿åç»­ä¸ç½‘ç»œè¾“å‡ºç›¸åŠ /å›å½’
        anchors = torch.log(anchors / (1 - anchors))
        # éæœ‰æ•ˆä½ç½®å¡«å……ä¸º infï¼ˆåç»­ä¼šè¢«å±è”½ï¼‰
        anchors = anchors.masked_fill(~valid_mask, float('inf'))
        return anchors, valid_mask

    def _get_encoder_input(self, x):
        # get projection features
        # å°†æ¯ä¸ªå°ºåº¦çš„ç‰¹å¾æŠ•å½±åˆ° hidden dim å¹¶å±•å¼€ä¸º [b, h*w, c]
        x = [self.input_proj[i](feat) for i, feat in enumerate(x)]
        feats = []
        shapes = []
        for feat in x:
            h, w = feat.shape[2:]
            # [b, c, h, w] -> [b, h*w, c]
            feats.append(feat.flatten(2).permute(0, 2, 1))
            # è®°å½•æ¯ä¸ªå°ºåº¦çš„ (h,w)
            shapes.append([h, w])

        # å°†ä¸åŒå°ºåº¦çš„ flattened features åœ¨ç©ºé—´ç»´åº¦æ‹¼æ¥ -> [b, sum(h*w), c]
        feats = torch.cat(feats, 1)
        return feats, shapes

    def _get_decoder_input(self, feats, shapes, dn_embed=None, dn_bbox=None):
        """ä» encoder è¾“å‡ºä¸­é€‰å– topk anchors ä½œä¸º decoder çš„ query referenceï¼ˆå‚è€ƒæ¡†ï¼‰ï¼Œå¹¶å‡†å¤‡ embeddingsã€‚

        è¿”å›ï¼š
        - embeddings: decoder çš„åˆå§‹è¾“å…¥ï¼ˆå¯åŒ…å« denoising çš„ embedï¼‰
        - refer_bbox: ç”¨äº decoder cross-attention çš„å‚è€ƒ bboxï¼ˆæœª sigmoidï¼‰
        - enc_bboxes: encoder side çš„ bboxï¼ˆsigmoid åï¼‰
        - enc_scores: encoder ä¾§çš„åˆ†ç±» logitsï¼ˆæœª sigmoidï¼‰
        """
        bs = len(feats)
        # ç”Ÿæˆ anchorsï¼ˆlogit ç©ºé—´ï¼‰å’Œ valid_mask
        anchors, valid_mask = self._generate_anchors(shapes, dtype=feats.dtype, device=feats.device)
        # å¯¹ encoder features åšä¸€ä¸ªçº¿æ€§ layer + layernormï¼ˆenc_outputï¼‰ï¼Œå¹¶ä¹˜ä»¥ valid_mask é¿å…æ— æ•ˆä½ç½®å½±å“
        features = self.enc_output(valid_mask * feats)  # bs, h*w, 256

        # encoder-side çš„åˆ†ç±»å’Œå›å½’è¾“å‡º
        enc_outputs_scores = self.enc_score_head(features)  # (bs, h*w, nc)
        enc_outputs_bboxes = self.enc_bbox_head(features) + anchors  # (bs, h*w, 4)  (netè¾“å‡º+anchors(logit))

        # query selection: æŒ‰ encoder é¢„æµ‹çš„æœ€å¤§ class score é€‰ topk positions
        topk_ind = torch.topk(enc_outputs_scores.max(-1).values, self.num_queries, dim=1).indices.view(-1)
        batch_ind = torch.arange(end=bs, dtype=topk_ind.dtype).unsqueeze(-1).repeat(1, self.num_queries).view(-1)

        # ä» enc_outputs_bboxes ä¸­æŒ‰é€‰ä¸­çš„ index å–å‡º refer_bboxï¼ˆæœª sigmoidï¼‰
        refer_bbox = enc_outputs_bboxes[batch_ind, topk_ind].view(bs, self.num_queries, -1)

        enc_bboxes = refer_bbox.sigmoid()  # encoder è¾“å‡ºçš„ bboxï¼ˆå¯ç”¨äºè®­ç»ƒç›‘ç£ï¼‰
        if dn_bbox is not None:
            # è‹¥ä½¿ç”¨ denoisingï¼Œåˆ™æŠŠ dn_bbox æ‹¼æ¥åœ¨ refer_bbox å‰é¢
            refer_bbox = torch.cat([dn_bbox, refer_bbox], 1)
        if self.training:
            # åœ¨è®­ç»ƒä¸­ detach refer_bbox ä»¥é˜»æ–­æ¢¯åº¦æµå› encoderï¼ˆç»éªŒåšæ³•ï¼‰
            refer_bbox = refer_bbox.detach()
        enc_scores = enc_outputs_scores[batch_ind, topk_ind].view(bs, self.num_queries, -1)

        # æ„é€  decoder çš„ embeddingsï¼šå¯ä»¥ä½¿ç”¨ learnable query æˆ– encoder ä¸­å¯¹åº” positions çš„ features
        if self.learnt_init_query:
            embeddings = self.tgt_embed.weight.unsqueeze(0).repeat(bs, 1, 1)
        else:
            embeddings = features[batch_ind, topk_ind].view(bs, self.num_queries, -1)
            if self.training:
                # åŒæ · detach embeddings åœ¨è®­ç»ƒé˜¶æ®µä»¥é¿å… encoder æ›´æ–°æ—¶äº§ç”Ÿä¸ç¨³å®š
                embeddings = embeddings.detach()
        if dn_embed is not None:
            embeddings = torch.cat([dn_embed, embeddings], 1)

        return embeddings, refer_bbox, enc_bboxes, enc_scores

    # TODO
    def _reset_parameters(self):
        """åˆå§‹åŒ–å„ head çš„æƒé‡ä¸åç½®ï¼Œéƒ¨åˆ†é‡‡ç”¨å¸¸æ•°åˆå§‹åŒ–ä»¥ä¾¿è®­ç»ƒç¨³å®šã€‚"""
        # åˆå§‹åŒ–ç±»åˆ«åç½®ä½¿å¾—åˆå§‹ sigmoid æ¦‚ç‡æ¥è¿‘æŸä¸ªå°å€¼ï¼ˆbias_init_with_prob ç”Ÿæˆä¸€ä¸ªå»ºè®®å€¼ï¼‰
        bias_cls = bias_init_with_prob(0.01) / 80 * self.nc
        # encoder head åˆå§‹åŒ–
        # ä½¿ç”¨å¸¸æ•°åˆå§‹åŒ–åˆ†ç±» bias
        constant_(self.enc_score_head.bias, bias_cls)
        # bbox head æœ€åä¸€å±‚ weight/bias åˆå§‹åŒ–ä¸º 0
        constant_(self.enc_bbox_head.layers[-1].weight, 0.)
        constant_(self.enc_bbox_head.layers[-1].bias, 0.)
        # decoder heads åˆå§‹åŒ–
        for cls_, reg_ in zip(self.dec_score_head, self.dec_bbox_head):
            constant_(cls_.bias, bias_cls)
            constant_(reg_.layers[-1].weight, 0.)
            constant_(reg_.layers[-1].bias, 0.)

        # çº¿æ€§å±‚ä¸ embedding åˆå§‹åŒ–ï¼ˆxavierï¼‰
        linear_init_(self.enc_output[0])
        xavier_uniform_(self.enc_output[0].weight)
        if self.learnt_init_query:
            xavier_uniform_(self.tgt_embed.weight)
        xavier_uniform_(self.query_pos_head.layers[0].weight)
        xavier_uniform_(self.query_pos_head.layers[1].weight)
        for layer in self.input_proj:
            xavier_uniform_(layer[0].weight)